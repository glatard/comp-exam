
@inproceedings{Chen2018-an,
  title     = {{Exploiting approximate computing for deep learning acceleration}},
  booktitle = {{2018 Design, Automation Test in Europe Conference Exhibition
               (DATE)}},
  author    = {Chen, Chia-Yu and Choi, Jungwook and Gopalakrishnan, Kailash and
               Srinivasan, Viji and Venkataramani, Swagath},
  abstract  = {Deep Neural Networks (DNNs) have emerged as a powerful and
               versatile set of techniques to address challenging artificial
               intelligence (AI) problems. Applications in domains such as
               image/video processing, natural language processing, speech
               synthesis and recognition, genomics and many others have
               embraced deep learning as the foundational technique. DNNs
               achieve superior accuracy for these applications using very
               large models which require 100s of MBs of data storage, ExaOps
               of computation and high bandwidth for data movement. Despite
               advances in computing systems, training state-of-the-art DNNs on
               large datasets takes several days/weeks, directly limiting the
               pace of innovation and adoption. In this paper, we discuss how
               these challenges can be addressed via approximate computing.
               Based on our earlier studies demonstrating that DNNs are
               resilient to numerical errors from approximate computing, we
               present techniques to reduce communication overhead of
               distributed deep learning training via adaptive residual
               gradient compression (AdaComp), and computation cost for deep
               learning inference via Prameterized clipping ACTivation (PACT)
               based network quantization. Experimental evaluation demonstrates
               order of magnitude savings in communication overhead for
               training and computational cost for inference while not
               compromising application accuracy.},
  pages     = {821--826},
  month     = mar,
  year      = 2018,
  keywords  = {Training;Computational modeling;Machine learning;Approximate
               computing;Quantization (signal);Data models;Convolution},
  issn      = {1558-1101},
  doi       = {10.23919/DATE.2018.8342119}
}

@article{Vicuna2021-mw,
  title         = {{Reducing numerical precision preserves classification
                   accuracy in Mondrian Forests}},
  author        = {Vicuna, Marc and Khannouz, Martin and Kiar, Gregory and
                   Chatelain, Yohan and Glatard, Tristan},
  abstract      = {Mondrian Forests are a powerful data stream classification
                   method, but their large memory footprint makes them
                   ill-suited for low-resource platforms such as connected
                   objects. We explored using reduced-precision floating-point
                   representations to lower memory consumption and evaluated
                   its effect on classification performance. We applied the
                   Mondrian Forest implementation provided by OrpailleCC, a C++
                   collection of data stream algorithms, to two canonical
                   datasets in human activity recognition: Recofit and Banos
                   \textbackslashemph\{et al\}. Results show that the precision
                   of floating-point values used by tree nodes can be reduced
                   from 64 bits to 8 bits with no significant difference in F1
                   score. In some cases, reduced precision was shown to improve
                   classification performance, presumably due to its
                   regularization effect. We conclude that numerical precision
                   is a relevant hyperparameter in the Mondrian Forest, and
                   that commonly-used double precision values may not be
                   necessary for optimal performance. Future work will evaluate
                   the generalizability of these findings to other data stream
                   classifiers.},
  month         = jun,
  year          = 2021,
  archiveprefix = {arXiv},
  eprint        = {2106.14340},
  primaryclass  = {cs.LG},
  arxivid       = {2106.14340}
}

@article{Salari2021-kd,
  title         = {{Accurate simulation of operating system updates in
                   neuroimaging using Monte-Carlo arithmetic}},
  author        = {Salari, Ali and Chatelain, Yohan and Kiar, Gregory and
                   Glatard, Tristan},
  abstract      = {Operating system (OS) updates introduce numerical
                   perturbations that impact the reproducibility of
                   computational pipelines. In neuroimaging, this has important
                   practical implications on the validity of computational
                   results, particularly when obtained in systems such as
                   high-performance computing clusters where the experimenter
                   does not control software updates. We present a framework to
                   reproduce the variability induced by OS updates in
                   controlled conditions. We hypothesize that OS updates impact
                   computational pipelines mainly through numerical
                   perturbations originating in mathematical libraries, which
                   we simulate using Monte-Carlo arithmetic in a framework
                   called ``fuzzy libmath'' (FL). We applied this methodology
                   to pre-processing pipelines of the Human Connectome Project,
                   a flagship open-data project in neuroimaging. We found that
                   FL-perturbed pipelines accurately reproduce the variability
                   induced by OS updates and that this similarity is only
                   mildly dependent on simulation parameters. Importantly, we
                   also found between-subject differences were preserved in
                   both cases, though the between-run variability was of
                   comparable magnitude for both FL and OS perturbations. We
                   found the numerical precision in the HCP pre-processed
                   images to be relatively low, with less than 8 significant
                   bits among the 24 available, which motivates further
                   investigation of the numerical stability of components in
                   the tested pipeline. Overall, our results establish that FL
                   accurately simulates results variability due to OS updates,
                   and is a practical framework to quantify numerical
                   uncertainty in neuroimaging.},
  month         = aug,
  year          = 2021,
  archiveprefix = {arXiv},
  eprint        = {2108.03129},
  primaryclass  = {q-bio.NC},
  arxivid       = {2108.03129}
}

@inproceedings{Chatelain2019-fu,
  title     = {{Automatic Exploration of Reduced Floating-Point Representations
               in Iterative Methods}},
  booktitle = {{Euro-Par 2019: Parallel Processing}},
  author    = {Chatelain, Yohan and Petit, Eric and de Oliveira Castro, Pablo
               and Lartigue, Ghislain and Defour, David},
  abstract  = {With the ever-increasing need for computation of scientific
               applications, new application domains, and major energy
               constraints, the landscape of floating-point computation is
               changing. New floating-point representation formats are emerging
               and there is a need for tools to simulate their impact in legacy
               codes. In this paper, we propose an automatic tool to evaluate
               the effect of adapting the floating point precision for each
               operation over time, which is particularly useful in iterative
               schemes. We present a backend to emulate any IEEE-754
               floating-point operation in lower precision. We tested the
               numerical errors resilience of our solutions thanks to Monte
               Carlo Arithmetic and demonstrated the effectiveness of this
               methodology on YALES2, a large Combustion-CFD HPC code, by
               achieving 28\% to 67\% reduction in communication volume by
               lowering precision.},
  publisher = {Springer International Publishing},
  pages     = {481--494},
  year      = 2019,
  doi       = {10.1007/978-3-030-29400-7\_34}
}

@inproceedings{Denis2016-ws,
  title     = {{Verificarlo: Checking Floating Point Accuracy through Monte
               Carlo Arithmetic}},
  booktitle = {{2016 IEEE 23nd Symposium on Computer Arithmetic (ARITH)}},
  author    = {Denis, Christophe and De Oliveira Castro, Pablo and Petit, Eric},
  abstract  = {Numerical accuracy of floating point computation is a well
               studied topic which has not made its way to the end-user in
               scientific computing. Yet, it has become a critical issue with
               the recent requirements for code modernization to harness new
               highly parallel hardware and perform higher resolution
               computation. To democratize numerical accuracy analysis, it is
               important to propose tools and methodologies to study large use
               cases in a reliable and automatic way. In this paper, we propose
               verificarlo, an extension to the LLVM compiler to automatically
               use Monte Carlo Arithmetic in a transparent way for the
               end-user. It supports all the major languages including C, C++,
               and Fortran. Unlike source-to-source approaches, our
               implementation captures the influence of compiler optimizations
               on the numerical accuracy. We illustrate how Monte Carlo
               Arithmetic using the verificarlo tool outperforms the existing
               approaches on various use cases and is a step toward automatic
               numerical analysis.},
  pages     = {55--62},
  month     = jul,
  year      = 2016,
  keywords  = {Monte Carlo methods;Numerical models;Optimization;Computational
               modeling;Standards;Instruments;Hardware;floating point
               arithmetic;numerical analysis;Monte Carlo arithmetic;compilers},
  issn      = {1063-6889},
  doi       = {10.1109/ARITH.2016.31}
}

@article{Brun2021-rs,
  title    = {{A Study of the Effects and Benefits of Custom-Precision
              Mathematical Libraries for HPC Codes}},
  author   = {Brun, Emeric and Defour, David and de Oliveira Castro, Pablo and
              I{\c s}toan, Matei and Mancusi, Davide and Petit, Eric and
              Vaquet, Alan},
  abstract = {Mathematical libraries are typically developed for use with the
              fixed-width data-paths on processors and target common
              floating-point formats such as IEEE binary32 and binary64. To
              address the increasing energy consumption and throughput
              requirements of HPC, scientific computing and AI applications,
              libraries and hardware implementations now provide new
              floating-point formats, allowing mathematical function
              evaluations with different performance and accuracy trade-offs.
              In this article we present a methodology and its associated
              proof-of-concept tool to evaluate the benefits of custom accuracy
              of mathematical library calls in HPC and scientific computations.
              First, our tool collects for each call-site of a mathematical
              function the input- and output-data profile. Then, using a
              heuristic exploration algorithm, we estimate the minimal required
              accuracy by rounding the result to lower precisions. The data
              profile and accuracy measurement per call-site is used to
              speculatively select the mathematical function implementation
              with the most appropriate accuracy for a given scenario. We have
              tested the methodology with the Intel MKL Vector Math (VM)
              library, leveraging the predefined accuracy levels. We
              demonstrate the benefits of our approach on two real-world
              applications: SGP4, a satellite tracking application, and PATMOS,
              a Monte Carlo neutron transport code. The robustness of the
              methodology is estimated by measuring the numerical accuracy of
              the resulting optimized code, against user-defined criteria. We
              experiment and discuss generalization across data-sets and
              finally propose a speculative runtime implementation for PATMOS.
              The experiment provides an insight into the performance
              improvements achievable by leveraging the control of per-function
              call-site accuracy-mode execution of the Intel compiler SVML
              library. We show benefits from 13 to 55 percent in time reduction
              for the PATMOS use case.},
  journal  = {IEEE Transactions on Emerging Topics in Computing},
  volume   = 9,
  number   = 3,
  pages    = {1467--1478},
  month    = jul,
  year     = 2021,
  keywords = {
              Libraries;Tools;Hardware;Optimization;Satellites;Instruments;Heuristic
              algorithms;Floating-point;computer arithmetic;mathematical
              library;libm;custom precision;variable
              precision;mixed-precision;high-performance
              computing;HPC;scientific computing;optimization;profile-guided
              optimization;PGO},
  issn     = {2168-6750},
  doi      = {10.1109/TETC.2021.3070422}
}

@article{Denis2015-yb,
  title         = {{Verificarlo: checking floating point accuracy through Monte
                   Carlo Arithmetic}},
  author        = {Denis, Christophe and De Oliveira Castro, Pablo and Petit,
                   Eric},
  abstract      = {Numerical accuracy of floating point computation is a well
                   studied topic which has not made its way to the end-user in
                   scientific computing. Yet, it has become a critical issue
                   with the recent requirements for code modernization to
                   harness new highly parallel hardware and perform higher
                   resolution computation. To democratize numerical accuracy
                   analysis, it is important to propose tools and methodologies
                   to study large use cases in a reliable and automatic way. In
                   this paper, we propose verificarlo, an extension to the LLVM
                   compiler to automatically use Monte Carlo Arithmetic in a
                   transparent way for the end-user. It supports all the major
                   languages including C, C++, and Fortran. Unlike
                   source-to-source approaches, our implementation captures the
                   influence of compiler optimizations on the numerical
                   accuracy. We illustrate how Monte Carlo Arithmetic using the
                   verificarlo tool outperforms the existing approaches on
                   various use cases and is a step toward automatic numerical
                   analysis.},
  month         = sep,
  year          = 2015,
  archiveprefix = {arXiv},
  eprint        = {1509.01347},
  primaryclass  = {cs.MS},
  arxivid       = {1509.01347}
}

@article{Mateos-Perez2018-wx,
  title    = {{Structural neuroimaging as clinical predictor: A review of
              machine learning applications}},
  author   = {Mateos-P{\'e}rez, Jos{\'e} Mar{\'\i}a and Dadar, Mahsa and
              Lacalle-Aurioles, Mar{\'\i}a and Iturria-Medina, Yasser and
              Zeighami, Yashar and Evans, Alan C},
  abstract = {In this paper, we provide an extensive overview of machine
              learning techniques applied to structural magnetic resonance
              imaging (MRI) data to obtain clinical classifiers. We
              specifically address practical problems commonly encountered in
              the literature, with the aim of helping researchers improve the
              application of these techniques in future works. Additionally, we
              survey how these algorithms are applied to a wide range of
              diseases and disorders (e.g. Alzheimer's disease (AD),
              Parkinson's disease (PD), autism, multiple sclerosis, traumatic
              brain injury, etc.) in order to provide a comprehensive view of
              the state of the art in different fields.},
  journal  = {Neuroimage Clin},
  volume   = 20,
  pages    = {506--522},
  month    = aug,
  year     = 2018,
  keywords = {Alzheimer; Autism; Cross-validation; Ensembling; Machine
              learning; Multiple sclerosis; Neuroimaging; Parkinson; Predictive
              modeling; SVMs; Structural magnetic resonance imaging},
  language = {en},
  issn     = {2213-1582},
  pmid     = {30167371},
  doi      = {10.1016/j.nicl.2018.08.019},
  pmc      = {PMC6108077}
}

@article{Nielsen2020-wu,
  title    = {{Machine Learning With Neuroimaging: Evaluating Its Applications
              in Psychiatry}},
  author   = {Nielsen, Ashley N and Barch, Deanna M and Petersen, Steven E and
              Schlaggar, Bradley L and Greene, Deanna J},
  abstract = {Psychiatric disorders are complex, involving heterogeneous
              symptomatology and neurobiology that rarely involves the
              disruption of single, isolated brain structures. In an attempt to
              better describe and understand the complexities of psychiatric
              disorders, investigators have increasingly applied multivariate
              pattern classification approaches to neuroimaging data and in
              particular supervised machine learning methods. However,
              supervised machine learning approaches also come with unique
              challenges and trade-offs, requiring additional study design and
              interpretation considerations. The goal of this review is to
              provide a set of best practices for evaluating machine learning
              applications to psychiatric disorders. We discuss how to evaluate
              two common efforts: 1) making predictions that have the potential
              to aid in diagnosis, prognosis, and treatment and 2)
              interrogating the complex neurophysiological mechanisms
              underlying psychopathology. We focus here on machine learning as
              applied to functional connectivity with magnetic resonance
              imaging, as an example to ground discussion. We argue that for
              machine learning classification to have translational utility for
              individual-level predictions, investigators must ensure that the
              classification is clinically informative, independent of
              confounding variables, and appropriately assessed for both
              performance and generalizability. We contend that shedding light
              on the complex mechanisms underlying psychiatric disorders will
              require consideration of the unique utility, interpretability,
              and reliability of the neuroimaging features (e.g., regions,
              networks, connections) identified from machine learning
              approaches. Finally, we discuss how the rise of large, multisite,
              publicly available datasets may contribute to the utility of
              machine learning approaches in psychiatry.},
  journal  = {Biol Psychiatry Cogn Neurosci Neuroimaging},
  volume   = 5,
  number   = 8,
  pages    = {791--798},
  month    = aug,
  year     = 2020,
  keywords = {Computational psychiatry; Feature selection; Functional
              connectivity; Machine learning; Neurophysiological mechanisms;
              Prediction},
  language = {en},
  issn     = {2451-9030, 2451-9022},
  pmid     = {31982357},
  doi      = {10.1016/j.bpsc.2019.11.007}
}

@article{Varoquaux2014-bq,
  title    = {{How machine learning is shaping cognitive neuroimaging}},
  author   = {Varoquaux, Gael and Thirion, Bertrand},
  abstract = {Functional brain images are rich and noisy data that can capture
              indirect signatures of neural activity underlying cognition in a
              given experimental setting. Can data mining leverage them to
              build models of cognition? Only if it is applied to well-posed
              questions, crafted to reveal cognitive mechanisms. Here we review
              how predictive models have been used on neuroimaging data to ask
              new questions, i.e., to uncover new aspects of cognitive
              organization. We also give a statistical learning perspective on
              these progresses and on the remaining gaping holes.},
  journal  = {Gigascience},
  volume   = 3,
  pages    = {28},
  month    = nov,
  year     = 2014,
  keywords = {Cognition; Decoding; Encoding; Machine learning; Neuroimaging;
              fMRI},
  language = {en},
  issn     = {2047-217X},
  pmid     = {25405022},
  doi      = {10.1186/2047-217X-3-28},
  pmc      = {PMC4234525}
}

@article{Kohoutova2020-le,
  title    = {{Toward a unified framework for interpreting machine-learning
              models in neuroimaging}},
  author   = {Kohoutov{\'a}, Lada and Heo, Juyeon and Cha, Sungmin and Lee,
              Sungwoo and Moon, Taesup and Wager, Tor D and Woo, Choong-Wan},
  abstract = {Machine learning is a powerful tool for creating computational
              models relating brain function to behavior, and its use is
              becoming widespread in neuroscience. However, these models are
              complex and often hard to interpret, making it difficult to
              evaluate their neuroscientific validity and contribution to
              understanding the brain. For neuroimaging-based machine-learning
              models to be interpretable, they should (i) be comprehensible to
              humans, (ii) provide useful information about what mental or
              behavioral constructs are represented in particular brain
              pathways or regions, and (iii) demonstrate that they are based on
              relevant neurobiological signal, not artifacts or confounds. In
              this protocol, we introduce a unified framework that consists of
              model-, feature- and biology-level assessments to provide
              complementary results that support the understanding of how and
              why a model works. Although the framework can be applied to
              different types of models and data, this protocol provides
              practical tools and examples of selected analysis methods for a
              functional MRI dataset and multivariate pattern-based predictive
              models. A user of the protocol should be familiar with basic
              programming in MATLAB or Python. This protocol will help build
              more interpretable neuroimaging-based machine-learning models,
              contributing to the cumulative understanding of brain mechanisms
              and brain health. Although the analyses provided here constitute
              a limited set of tests and take a few hours to days to complete,
              depending on the size of data and available computational
              resources, we envision the process of annotating and interpreting
              models as an open-ended process, involving collaborative efforts
              across multiple studies and laboratories.},
  journal  = {Nat. Protoc.},
  volume   = 15,
  number   = 4,
  pages    = {1399--1435},
  month    = apr,
  year     = 2020,
  language = {en},
  issn     = {1754-2189, 1750-2799},
  pmid     = {32203486},
  doi      = {10.1038/s41596-019-0289-5}
}

@article{Jollans2019-nc,
  title    = {{Quantifying performance of machine learning methods for
              neuroimaging data}},
  author   = {Jollans, Lee and Boyle, Rory and Artiges, Eric and Banaschewski,
              Tobias and Desrivi{\`e}res, Sylvane and Grigis, Antoine and
              Martinot, Jean-Luc and Paus, Tom{\'a}{\v s} and Smolka, Michael N
              and Walter, Henrik and Schumann, Gunter and Garavan, Hugh and
              Whelan, Robert},
  abstract = {Machine learning is increasingly being applied to neuroimaging
              data. However, most machine learning algorithms have not been
              designed to accommodate neuroimaging data, which typically has
              many more data points than subjects, in addition to
              multicollinearity and low signal-to-noise. Consequently, the
              relative efficacy of different machine learning regression
              algorithms for different types of neuroimaging data are not
              known. Here, we sought to quantify the performance of a variety
              of machine learning algorithms for use with neuroimaging data
              with various sample sizes, feature set sizes, and predictor
              effect sizes. The contribution of additional machine learning
              techniques - embedded feature selection and bootstrap aggregation
              (bagging) - to model performance was also quantified. Five
              machine learning regression methods - Gaussian Process
              Regression, Multiple Kernel Learning, Kernel Ridge Regression,
              the Elastic Net and Random Forest, were examined with both real
              and simulated MRI data, and in comparison to standard multiple
              regression. The different machine learning regression algorithms
              produced varying results, which depended on sample size, feature
              set size, and predictor effect size. When the effect size was
              large, the Elastic Net, Kernel Ridge Regression and Gaussian
              Process Regression performed well at most sample sizes and
              feature set sizes. However, when the effect size was small, only
              the Elastic Net made accurate predictions, but this was limited
              to analyses with sample sizes greater than 400. Random Forest
              also produced a moderate performance for small effect sizes, but
              could do so across all sample sizes. Machine learning techniques
              also improved prediction accuracy for multiple regression. These
              data provide empirical evidence for the differential performance
              of various machines on neuroimaging data, which are dependent on
              number of sample size, features and effect size.},
  journal  = {Neuroimage},
  volume   = 199,
  pages    = {351--365},
  month    = oct,
  year     = 2019,
  keywords = {Machine learning; Neuroimaging; Regression algorithms;
              Reproducibility},
  language = {en},
  issn     = {1053-8119, 1095-9572},
  pmid     = {31173905},
  doi      = {10.1016/j.neuroimage.2019.05.082},
  pmc      = {PMC6688909}
}

@article{Abraham2014-zv,
  title    = {{Machine learning for neuroimaging with scikit-learn}},
  author   = {Abraham, Alexandre and Pedregosa, Fabian and Eickenberg, Michael
              and Gervais, Philippe and Mueller, Andreas and Kossaifi, Jean and
              Gramfort, Alexandre and Thirion, Bertrand and Varoquaux, Ga{\"e}l},
  abstract = {Statistical machine learning methods are increasingly used for
              neuroimaging data analysis. Their main virtue is their ability to
              model high-dimensional datasets, e.g., multivariate analysis of
              activation images or resting-state time series. Supervised
              learning is typically used in decoding or encoding settings to
              relate brain images to behavioral or clinical observations, while
              unsupervised learning can uncover hidden structures in sets of
              images (e.g., resting state functional MRI) or find
              sub-populations in large cohorts. By considering different
              functional neuroimaging applications, we illustrate how
              scikit-learn, a Python machine learning library, can be used to
              perform some key analysis steps. Scikit-learn contains a very
              large set of statistical learning algorithms, both supervised and
              unsupervised, and its application to neuroimaging data provides a
              versatile tool to study the brain.},
  journal  = {Front. Neuroinform.},
  volume   = 8,
  pages    = {14},
  month    = feb,
  year     = 2014,
  keywords = {Python; machine learning; neuroimaging; scikit-learn; statistical
              learning},
  language = {en},
  issn     = {1662-5196},
  pmid     = {24600388},
  doi      = {10.3389/fninf.2014.00014}
}

@article{Davatzikos2019-dc,
  title    = {{Machine learning in neuroimaging: Progress and challenges}},
  author   = {Davatzikos, Christos},
  journal  = {Neuroimage},
  volume   = 197,
  pages    = {652--656},
  month    = aug,
  year     = 2019,
  language = {en},
  issn     = {1053-8119, 1095-9572},
  pmid     = {30296563},
  doi      = {10.1016/j.neuroimage.2018.10.003},
  pmc      = {PMC6499712}
}

@book{Muller2018-zm,
  title     = {{Handbook of Floating-Point Arithmetic}},
  author    = {Muller, Jean-Michel and Brunie, Nicolas and de Dinechin, Florent
               and Jeannerod, Claude-Pierre and Joldes, Mioara and Lef{\`e}vre,
               Vincent and Melquiond, Guillaume and Revol, Nathalie and Torres,
               Serge},
  publisher = {Birkh{\"a}user, Cham},
  year      = 2018,
  isbn      = {9783319765259, 9783319765266},
  doi       = {10.1007/978-3-319-76526-6}
}

@article{Goldberg1991-nv,
  title     = {{What every computer scientist should know about floating-point
               arithmetic}},
  author    = {Goldberg, David},
  abstract  = {Floating-point arithmetic is considered as esoteric subject by
               many people. This is rather surprising, because floating-point
               is ubiquitous in computer systems: Almost every language has a
               floating-point datatype; computers from PCs to supercomputers
               have floating-point accelerators; most compilers will be called
               upon to compile floating-point algorithms from time to time; and
               virtually every operating system must respond to floating-point
               exceptions such as overflow. This paper presents a tutorial on
               the aspects of floating-point that have a direct impact on
               designers of computer systems. It begins with background on
               floating-point representation and rounding error, continues with
               a discussion of the IEEE floating point standard, and concludes
               with examples of how computer system builders can better support
               floating point.},
  journal   = {ACM Comput. Surv.},
  publisher = {Association for Computing Machinery},
  volume    = 23,
  number    = 1,
  pages     = {5--48},
  month     = mar,
  year      = 1991,
  address   = {New York, NY, USA},
  keywords  = {relative error, NaN, underflow, floating-point, ulp, gradual
               underflow, rounding error, denormalized number, floating-point
               standard, exception, guard digit, rounding mode, overflow},
  issn      = {0360-0300},
  doi       = {10.1145/103162.103163}
}

@article{Cherubin2020-tt,
  title     = {{Tools for Reduced Precision Computation: A Survey}},
  author    = {Cherubin, Stefano and Agosta, Giovanni},
  abstract  = {The use of reduced precision to improve performance metrics such
               as computation latency and power consumption is a common
               practice in the embedded systems field. This practice is
               emerging as a new trend in High Performance Computing (HPC),
               especially when new error-tolerant applications are considered.
               However, standard compiler frameworks do not support automated
               precision customization, and manual tuning and code
               transformation is the approach usually adopted in most domains.
               In recent years, research have been studying ways to improve the
               automation of this process. This article surveys this body of
               work, identifying the critical steps of this process, the most
               advanced tools available, and the open challenges in this
               research area. We conclude that, while several mature tools
               exist, there is still a gap to close, especially for tools based
               on static analysis rather than profiling, as well as for
               integration within mainstream, industry-strength compiler
               frameworks.},
  journal   = {ACM Comput. Surv.},
  publisher = {Association for Computing Machinery},
  volume    = 53,
  number    = 2,
  pages     = {1--35},
  month     = apr,
  year      = 2020,
  address   = {New York, NY, USA},
  keywords  = {Reduced precision, approximate computing},
  issn      = {0360-0300},
  doi       = {10.1145/3381039}
}

@article{Nguyen2018-lo,
  title         = {{False discovery rate control under reduced precision
                   computation for analysis of neuroimaging data}},
  author        = {Nguyen, Hien D and Yee, Yohan and McLachlan, Geoffrey J and
                   Lerch, Jason P},
  abstract      = {The mitigation of false positives is an important issue when
                   conducting multiple hypothesis testing. The most popular
                   paradigm for false positives mitigation in high-dimensional
                   applications is via the control of the false discovery rate
                   (FDR). Multiple testing data from neuroimaging experiments
                   can be very large, and reduced precision storage of such
                   data is often required. Reduced precision computation is
                   often a problem in the analysis of legacy data and data
                   arising from legacy pipelines. We present a method for FDR
                   control that is applicable in cases where only
                   p\textbackslashtext\{-values\} or test statistics (with
                   common and known null distribution) are available, and when
                   those p\textbackslashtext\{-values\} or test statistics are
                   encoded in a reduced precision format. Our method is based
                   on an empirical-Bayes paradigm where the probit
                   transformation of the p\textbackslashtext\{-values\} (called
                   the z\textbackslashtext\{-scores\}) are modeled as a
                   two-component mixture of normal distributions. Due to the
                   reduced precision of the p\textbackslashtext\{-values\} or
                   test statistics, the usual approach for fitting mixture
                   models may not be feasible. We instead use a binned-data
                   technique, which can be proved to consistently estimate the
                   z\textbackslashtext\{-score\} distribution parameters under
                   mild correlation assumptions, as is often the case in
                   neuroimaging data. A simulation study shows that our
                   methodology is competitive when compared with popular
                   alternatives, especially with data in the presence of
                   misspecification. We demonstrate the applicability of our
                   methodology in practice via a brain imaging study of mice.},
  month         = may,
  year          = 2018,
  archiveprefix = {arXiv},
  eprint        = {1805.04394},
  primaryclass  = {stat.ME},
  arxivid       = {1805.04394}
}

@article{Chougar2021-fk,
  title    = {{Automated Categorization of Parkinsonian Syndromes Using Magnetic
              Resonance Imaging in a Clinical Setting}},
  author   = {Chougar, Lydia and Faouzi, Johann and Pyatigorskaya, Nadya and
              Yahia-Cherif, Lydia and Gaurav, Rahul and Biondetti, Emma and
              Villotte, Marie and Valabr{\`e}gue, Romain and Corvol,
              Jean-Christophe and Brice, Alexis and Mariani, Louise-Laure and
              Cormier, Florence and Vidailhet, Marie and Dupont, Gwendoline and
              Piot, Ines and Grabli, David and Payan, Christine and Colliot,
              Olivier and Degos, Bertrand and Leh{\'e}ricy, St{\'e}phane},
  abstract = {BACKGROUND: Machine learning algorithms using magnetic resonance
              imaging (MRI) data can accurately discriminate parkinsonian
              syndromes. Validation in patients recruited in routine clinical
              practice is missing. OBJECTIVE: The aim of this study was to
              assess the accuracy of a machine learning algorithm trained on a
              research cohort and tested on an independent clinical replication
              cohort for the categorization of parkinsonian syndromes. METHODS:
              Three hundred twenty-two subjects, including 94 healthy control
              subjects, 119 patients with Parkinson's disease (PD), 51 patients
              with progressive supranuclear palsy (PSP) with Richardson's
              syndrome, 35 with multiple system atrophy (MSA) of the
              parkinsonian variant (MSA-P), and 23 with MSA of the cerebellar
              variant (MSA-C), were recruited. They were divided into a
              training cohort (n = 179) scanned in a research environment and a
              replication cohort (n = 143) examined in clinical practice on
              different MRI systems. Volumes and diffusion tensor imaging (DTI)
              metrics in 13 brain regions were used as input for a supervised
              machine learning algorithm. To harmonize data across scanners and
              reduce scanner-dependent effects, we tested two types of
              normalizations using patient data or healthy control data.
              RESULTS: In the replication cohort, high accuracies were achieved
              using volumetry in the classification of PD-PSP, PD-MSA-C,
              PSP-MSA-C, and PD-atypical parkinsonism (balanced accuracies:
              0.840-0.983, area under the receiver operating characteristic
              curves: 0.907-0.995). Performances were lower for the
              classification of PD-MSA-P, MSA-C-MSA-P (balanced accuracies:
              0.765-0.784, area under the receiver operating characteristic
              curve: 0.839-0.871) and PD-PSP-MSA (balanced accuracies: 0.773).
              Performance using DTI was improved when normalizing by controls,
              but remained lower than that using volumetry alone or combined
              with DTI. CONCLUSIONS: A machine learning approach based on
              volumetry enabled accurate classification of subjects with
              early-stage parkinsonism, examined on different MRI systems, as
              part of their clinical assessment. \copyright{} 2020
              International Parkinson and Movement Disorder Society.},
  journal  = {Mov. Disord.},
  volume   = 36,
  number   = 2,
  pages    = {460--470},
  month    = feb,
  year     = 2021,
  keywords = {Parkinson's disease; machine learning algorithm; multimodal
              magnetic resonance imaging; multiple system atrophy; progressive
              supranuclear palsy},
  language = {en},
  issn     = {0885-3185, 1531-8257},
  pmid     = {33137232},
  doi      = {10.1002/mds.28348}
}

@inproceedings{Carmichael2019-nu,
  title     = {{Performance-Efficiency Trade-off of Low-Precision Numerical
               Formats in Deep Neural Networks}},
  booktitle = {{Proceedings of the Conference for Next Generation Arithmetic
               2019}},
  author    = {Carmichael, Zachariah and Langroudi, Hamed F and Khazanov, Char
               and Lillie, Jeffrey and Gustafson, John L and Kudithipudi,
               Dhireesha},
  abstract  = {Deep neural networks (DNNs) have been demonstrated as effective
               prognostic models across various domains, e.g. natural language
               processing, computer vision, and genomics. However, modern-day
               DNNs demand high compute and memory storage for executing any
               reasonably complex task. To optimize the inference time and
               alleviate the power consumption of these networks, DNN
               accelerators with low-precision representations of data and DNN
               parameters are being actively studied. An interesting research
               question is in how low-precision networks can be ported to
               edge-devices with similar performance as high-precision
               networks. In this work, we employ the fixed-point, floating
               point, and posit numerical formats at $\leq$8-bit precision
               within a DNN accelerator, Deep Positron, with exact
               multiply-and-accumulate (EMAC) units for inference. A unified
               analysis quantifies the trade-offs between overall network
               efficiency and performance across five classification tasks. Our
               results indicate that posits are a natural fit for DNN
               inference, outperforming at $\leq$8-bit precision, and can be
               realized with competitive resource requirements relative to
               those of floating point.},
  publisher = {Association for Computing Machinery},
  number    = {Article 3},
  pages     = {1--9},
  series    = {CoNGA'19},
  month     = mar,
  year      = 2019,
  address   = {New York, NY, USA},
  keywords  = {posit numerical format, machine learning, deep neural networks,
               DNN accelerators, tapered precision, low-precision, floating
               point},
  location  = {Singapore, Singapore},
  isbn      = {9781450371391},
  doi       = {10.1145/3316279.3316282}
}

@article{Judd2015-kw,
  title         = {{Reduced-Precision Strategies for Bounded Memory in Deep
                   Neural Nets}},
  author        = {Judd, Patrick and Albericio, Jorge and Hetherington, Tayler
                   and Aamodt, Tor and Jerger, Natalie Enright and Urtasun,
                   Raquel and Moshovos, Andreas},
  abstract      = {This work investigates how using reduced precision data in
                   Convolutional Neural Networks (CNNs) affects network
                   accuracy during classification. More specifically, this
                   study considers networks where each layer may use different
                   precision data. Our key result is the observation that the
                   tolerance of CNNs to reduced precision data not only varies
                   across networks, a well established observation, but also
                   within networks. Tuning precision per layer is appealing as
                   it could enable energy and performance improvements. In this
                   paper we study how error tolerance across layers varies and
                   propose a method for finding a low precision configuration
                   for a network while maintaining high accuracy. A diverse set
                   of CNNs is analyzed showing that compared to a conventional
                   implementation using a 32-bit floating-point representation
                   for all layers, and with less than 1\% loss in relative
                   accuracy, the data footprint required by these networks can
                   be reduced by an average of 74\% and up to 92\%.},
  month         = nov,
  year          = 2015,
  archiveprefix = {arXiv},
  eprint        = {1511.05236},
  primaryclass  = {cs.LG},
  arxivid       = {1511.05236}
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@article{Lesser_2011,
  title     = {{Effects of Reduced Precision on Floating-Point SVM
               Classification Accuracy}},
  abstract  = {There is growing interest in performing ever more complex
               classification tasks on mobile and embedded devices in
               real-time, which results in the need …},
  journal   = {Procedia Comput. Sci.},
  publisher = {Elsevier},
  volume    = 4,
  pages     = {508--517},
  month     = jan,
  year      = 2011,
  issn      = {1877-0509},
  doi       = {10.1016/j.procs.2011.04.053}
}

@article{Higham2019-yd,
  title     = {{Simulating Low Precision Floating-Point Arithmetic}},
  author    = {Higham, Nicholas J and Pranesh, Srikara},
  abstract  = {The half-precision (fp16) floating-point format, defined in the
               2008 revision of the IEEE standard for floating-point
               arithmetic, and a more recently proposed half-precision format
               bfloat16, are increasingly available in GPUs and other
               accelerators. While the support for low precision arithmetic is
               mainly motivated by machine learning applications, general
               purpose numerical algorithms can benefit from it, too, gaining
               in speed, energy usage, and reduced communication costs. Since
               the appropriate hardware is not always available, and one may
               wish to experiment with new arithmetics not yet implemented in
               hardware, software simulations of low precision arithmetic are
               needed. We discuss how to simulate low precision arithmetic
               using arithmetic of higher precision. We examine the correctness
               of such simulations and explain via rounding error analysis why
               a natural method of simulation can provide results that are more
               accurate than actual computations at low precision. We provide a
               MATLAB function, chop, that can be used to efficiently simulate
               fp16, bfloat16, and other low precision arithmetics, with or
               without the representation of subnormal numbers and with the
               options of round to nearest, directed rounding, stochastic
               rounding, and random bit flips in the significand. We
               demonstrate the advantages of this approach over defining a new
               MATLAB class and overloading operators.},
  journal   = {SIAM J. Sci. Comput.},
  publisher = {Society for Industrial and Applied Mathematics},
  volume    = 41,
  number    = 5,
  pages     = {C585--C602},
  month     = jan,
  year      = 2019,
  issn      = {1064-8275},
  doi       = {10.1137/19M1251308}
}

@article{Zhang2019-xv,
  title         = {{QPyTorch: A Low-Precision Arithmetic Simulation Framework}},
  author        = {Zhang, Tianyi and Lin, Zhiqiu and Yang, Guandao and De Sa,
                   Christopher},
  abstract      = {Low-precision training reduces computational cost and
                   produces efficient models. Recent research in developing new
                   low-precision training algorithms often relies on simulation
                   to empirically evaluate the statistical effects of
                   quantization while avoiding the substantial overhead of
                   building specific hardware. To support this empirical
                   research, we introduce QPyTorch, a low-precision arithmetic
                   simulation framework. Built natively in PyTorch, QPyTorch
                   provides a convenient interface that minimizes the efforts
                   needed to reliably convert existing codes to study
                   low-precision training. QPyTorch is general, and supports a
                   variety of combinations of precisions, number formats, and
                   rounding options. Additionally, it leverages an efficient
                   fused-kernel approach to reduce simulator overhead, which
                   enables simulation of large-scale, realistic problems.
                   QPyTorch is publicly available at
                   https://github.com/Tiiiger/QPyTorch.},
  month         = oct,
  year          = 2019,
  archiveprefix = {arXiv},
  eprint        = {1910.04540},
  primaryclass  = {cs.LG},
  arxivid       = {1910.04540}
}

@article{noauthor_undated-rg,
  title = {{REUSE OF HIGH PRECISION ARITHMETIC HARDWARE TO PERFORM MULTIPLE
           CONCURRENT LOW PRECISION CALCULATIONS}}
}

@article{Wang2018-oo,
  title         = {{Training Deep Neural Networks with 8-bit floating point
                   numbers}},
  author        = {Wang, Naigang and Choi, Jungwook and Brand, Daniel and Chen,
                   Chia-Yu and Gopalakrishnan, Kailash},
  abstract      = {The state-of-the-art hardware platforms for training Deep
                   Neural Networks (DNNs) are moving from traditional single
                   precision (32-bit) computations towards 16 bits of precision
                   -- in large part due to the high energy efficiency and
                   smaller bit storage associated with using reduced-precision
                   representations. However, unlike inference, training with
                   numbers represented with less than 16 bits has been
                   challenging due to the need to maintain fidelity of the
                   gradient computations during back-propagation. Here we
                   demonstrate, for the first time, the successful training of
                   DNNs using 8-bit floating point numbers while fully
                   maintaining the accuracy on a spectrum of Deep Learning
                   models and datasets. In addition to reducing the data and
                   computation precision to 8 bits, we also successfully reduce
                   the arithmetic precision for additions (used in partial
                   product accumulation and weight updates) from 32 bits to 16
                   bits through the introduction of a number of key ideas
                   including chunk-based accumulation and floating point
                   stochastic rounding. The use of these novel techniques lays
                   the foundation for a new generation of hardware training
                   platforms with the potential for 2-4x improved throughput
                   over today's systems.},
  month         = dec,
  year          = 2018,
  copyright     = {http://arxiv.org/licenses/nonexclusive-distrib/1.0/},
  archiveprefix = {arXiv},
  eprint        = {1812.08011},
  primaryclass  = {cs.LG},
  arxivid       = {1812.08011}
}

@inproceedings{Anderson2016-yn,
  title     = {{Vectorization of multibyte floating point data formats}},
  booktitle = {{2016 International Conference on Parallel Architecture and
               Compilation Techniques (PACT)}},
  author    = {Anderson, Andrew and Gregg, David},
  abstract  = {We propose a scheme for reduced-precision representation of
               floating point data on a continuum between IEEE-754 floating
               point types. Our scheme enables the use of lower precision
               formats for a reduction in storage space requirements and data
               transfer volume. We describe how our scheme can be accelerated
               using existing hardware vector units on a general-purpose
               processor (GPP). Exploiting native vector hardware allows us to
               support reduced precision floating point with low overhead. We
               demonstrate that supporting reduced precision in the compiler as
               opposed to using a library approach can yield a low overhead
               solution for GPPs.},
  pages     = {363--372},
  month     = sep,
  year      = 2016,
  keywords  = {Hardware;Standards;Writing;Software;Approximation
               algorithms;Memory management;Approximate Computing;Floating
               Point;Multiple Precision;SIMD;Vector Architecture},
  doi       = {10.1145/2967938.2967966}
}

@article{Johnson2018-up,
  title         = {{Rethinking floating point for deep learning}},
  author        = {Johnson, Jeff},
  abstract      = {Reducing hardware overhead of neural networks for faster or
                   lower power inference and training is an active area of
                   research. Uniform quantization using integer multiply-add
                   has been thoroughly investigated, which requires learning
                   many quantization parameters, fine-tuning training or other
                   prerequisites. Little effort is made to improve floating
                   point relative to this baseline; it remains energy
                   inefficient, and word size reduction yields drastic loss in
                   needed dynamic range. We improve floating point to be more
                   energy efficient than equivalent bit width integer hardware
                   on a 28 nm ASIC process while retaining accuracy in 8 bits
                   with a novel hybrid log multiply/linear add, Kulisch
                   accumulation and tapered encodings from Gustafson's posit
                   format. With no network retraining, and drop-in replacement
                   of all math and float32 parameters via round-to-nearest-even
                   only, this open-sourced 8-bit log float is within 0.9\%
                   top-1 and 0.2\% top-5 accuracy of the original float32
                   ResNet-50 CNN model on ImageNet. Unlike int8 quantization,
                   it is still a general purpose floating point arithmetic,
                   interpretable out-of-the-box. Our 8/38-bit log float
                   multiply-add is synthesized and power profiled at 28 nm at
                   0.96x the power and 1.12x the area of 8/32-bit integer
                   multiply-add. In 16 bits, our log float multiply-add is
                   0.59x the power and 0.68x the area of IEEE 754 float16 fused
                   multiply-add, maintaining the same signficand precision and
                   dynamic range, proving useful for training ASICs as well.},
  month         = nov,
  year          = 2018,
  archiveprefix = {arXiv},
  eprint        = {1811.01721},
  primaryclass  = {cs.NA},
  arxivid       = {1811.01721}
}


@article{Gustafson2017-wo,
  title     = {{Beating Floating Point at its Own Game: Posit Arithmetic}},
  author    = {{Gustafson} and {Yonemoto}},
  abstract  = {A new data type called a posit is designed as a direct drop-in
               replacement for IEEE Standard 754 floating-point numbers floats.
               Unlike earlier forms of universal number unum arithmetic, posits
               do not require interval arithmetic or variable size operands;
               like floats, they round if an answer is inexact. However, they
               provide compelling advantages over floats, including larger
               dynamic range, higher accuracy, better closure, bitwise
               identical results across systems, simpler hardware, and simpler
               exception handling. Posits never overflow to infinity or
               underflow to zero, and ``Nota-Number'' NaN indicates an action
               instead of a bit pattern. A posit processing unit takes less
               circuitry than an IEEE float FPU. With lower power use and
               smaller silicon footprint, the posit operations per second POPS
               supported by a chip can be significantly higher than the FLOPS
               using similar hardware resources. GPU accelerators and Deep
               Learning processors, in particular, can do more per watt and per
               dollar with posits, yet deliver superior answer quality. A
               comprehensive series of benchmarks compares floats and posits
               for decimals of accuracy produced for a set precision. Low
               precision posits provide a better solution than ``approximate
               computing'' methods that try to tolerate decreased answer
               quality. High precision posits provide more correct decimals
               than floats of the same size; in some cases, a 32-bit posit may
               safely replace a 64-bit float. In other words, posits beat
               floats at their own game.},
  journal   = {Supercomput. Front. Innov.: Int. J.},
  publisher = {South Ural State University},
  volume    = 4,
  number    = 2,
  pages     = {71--86},
  month     = jun,
  year      = 2017,
  address   = {Chelyabinsk, RUS},
  keywords  = {unum computing, posits, neural networks, linear algebra,
               LINPACK, floating point, energy-efficient computing, computer
               arithmetic, valid arithmetic},
  issn      = {2409-6008},
  doi       = {10.14529/jsfi170206}
}

@article{PyTorch_2019,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {8024--8035},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{tensorflow2015-whitepaper,
  title  = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  url    = {http://tensorflow.org/},
  note   = {Software available from tensorflow.org},
  author = {
            Mart\'{\i}n~Abadi and
            Ashish~Agarwal and
            Paul~Barham and
            Eugene~Brevdo and
            Zhifeng~Chen and
            Craig~Citro and
            Greg~S.~Corrado and
            Andy~Davis and
            Jeffrey~Dean and
            Matthieu~Devin and
            Sanjay~Ghemawat and
            Ian~Goodfellow and
            Andrew~Harp and
            Geoffrey~Irving and
            Michael~Isard and
            Yangqing Jia and
            Rafal~Jozefowicz and
            Lukasz~Kaiser and
            Manjunath~Kudlur and
            Josh~Levenberg and
            Dan~Man\'{e} and
            Rajat~Monga and
            Sherry~Moore and
            Derek~Murray and
            Chris~Olah and
            Mike~Schuster and
            Jonathon~Shlens and
            Benoit~Steiner and
            Ilya~Sutskever and
            Kunal~Talwar and
            Paul~Tucker and
            Vincent~Vanhoucke and
            Vijay~Vasudevan and
            Fernanda~Vi\'{e}gas and
            Oriol~Vinyals and
            Pete~Warden and
            Martin~Wattenberg and
            Martin~Wicke and
            Yuan~Yu and
            Xiaoqiang~Zheng},
  year   = {2015}
}

@misc{bfloat16,
  title     = {The BFLOAT16 Numerical Format},
  url       = {https://cloud.google.com/tpu/docs/bfloat16},
  journal   = {Google},
  publisher = {Google},
  author    = {Google},
  year      = {2021},
  month     = {Nov}
} 

@misc{tpu,
  title     = {Cloud Tensor Processing Units (TPUs)},
  url       = {https://cloud.google.com/tpu/docs/tpus},
  journal   = {Google},
  publisher = {Google},
  author    = {Google},
  year      = {2021},
  month     = {Nov}
} 
