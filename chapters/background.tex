\chapter{Floating-points}
\begin{comment}
This section will discuss:
- Floating-point
* Definitions and basic notions
* IEEE-754
* Sources of error
\end{comment}
Among the different methods to represent the set of real numbers in computer,
floating-points is a commonly adopted format.
This section discuss some fundamental definitions and basic notions on
floating-points, briefly describe the \textit{IEEE-754} data format, and 
explains the sources of errors in floating-point arithmethic and their consequences.

\section{Definitions and basic notions}
% 1.5 page
\begin{comment}
- Properties of float arithmetic
\end{comment}
A floating-point format is defined with four intergers:
\begin{itemize}
	\item A \textit{radix} (or \textit{Base}) $B \ge 2$.
	\item A \textit{precision} $p \ge 2$ which approximately represents the number of significant digits for the format.
	\item Two exponent $e_{min}$ and $e_{max}$ that bound the range of value represent by the format. In practice, $e_{min} \le 0 \le e_{max}$.
\end{itemize}

A floating-point number in such a format is a number $x$ that can be represented by a pair $(M,e)$, such that
\begin{equation}
	x = M \cdot B^{e-p+1}
\end{equation}
where
\begin{itemize}
	\item $M$ is an integer such that $|M| \le B^{p}-1$. It is called the \textit{integral siginificand} of x.
	\item $e$ is an integer such that $e_{min} \le e \le e_{max}$. It is called the \textit{exponent} of x.
\end{itemize}
The representation of a number using a pair $(M, e)$ is not unique.
The set of all such representation is called a \textit{cohort}.
It is often desirable to have a unique representation; i.e. called \textit{normalized representation}.
This simplifies the expression of error bounds and the implementation.
It can be achieved, for example, by always representing numbers using the minimum exponent possible.
Floating-point are often categorized into \textit{normal} and \textit{subnormal}.
In radix 2, the first digits of a siginificand is 1 for normal number and 0 for subnormal.
This allows the usage of encoding strategies such as \textit{hidden bit} to
ignore the most significand bit without losing precision.
Moreover, the availability of subnormal numbers allows \textit{gradual underflow},
which helps to implement numerically stable softwares.

A common way to define the error introduced by an floating-point arithmethic
operation is using the \textit{unit in the last place (ulp)} definition.
The $ulp(x)$ is defined as the distance between the two closest distinct
floating-point numbers $a$ and $b$, such that $a < x < b$ and $a \neq b$.
%  TODO Add a figure that depicts the ulp definition.

A function is \textit{correctly rounded} when its results are always rounded the 
same way as if it used infinite precision and range.
In other word, a function is correctly rounded when its error is within $0.5$ ulp.
When a function cannot guarantee correct rounding and always round a number $y$
using one of \textit{round-up} or \textit{round-down} funciton, it is said to be \textit{faithful}.
Correct rounding functions are always faithful.
One must keep in mind that even if a function is correctly rounded, there is
still loss of information is the number cannot be represented exactly in the
floating-poitn format.

With floating-point arithmetic, some properties from real number arithmethic are lost while other remain.
Any correctly rounded arithmetic operations remain commutative with addition and multiplication.
However, associativity and distributivity no longer apply.
If not cautious about this, it can result in drastic errors.


\section{IEEE-754 binary format}
%  2 page
\begin{comment}
    
\end{comment}


\section{Sources of error}
% 1 page
\begin{comment}
- rounding
- double rounding
- cancellation
- overflow & underflow
- accumulation
\end{comment}
